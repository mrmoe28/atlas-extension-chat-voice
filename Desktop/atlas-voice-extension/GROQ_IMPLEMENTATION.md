# Groq Integration Implementation - Session Handoff

## âœ… Completed in This Session

### 1. Fixed All 500 Server Errors
- **Issue**: OPENAI_API_KEY missing from Vercel environments
- **Solution**: Added key to Production, Preview, and Development environments
- **Result**: `/api/ephemeral` and `/api/vision` endpoints now work

### 2. Fixed Production URL Routing
- **Issue**: atlas-voice-extension.vercel.app pointed to old deployment without API key
- **Solution**: Updated Vercel alias to point to correct deployment
- **Command Used**: `vercel alias set https://server-ijhkso2g8-ekoapps.vercel.app atlas-voice-extension.vercel.app`
- **Result**: Production URL now returns valid OpenAI credentials

### 3. Implemented Groq API Backend
- **File**: `dev/server/server.js` (lines 512-598)
- **Endpoint**: `POST /api/groq`
- **Model**: `llama-3.3-70b-versatile`
- **Environment Variable**: `GROQ_API_KEY` added to all Vercel environments
- **Test Result**: 193 tokens in 0.27 seconds, $0.00 cost (free tier)
- **API Key**: Already configured in Vercel (see .env.local or Vercel dashboard)

### 4. Added UI for AI Provider Selection
- **File**: `sidepanel.html` (lines 116-128)
- **Options**:
  - Groq + Browser Speech (FREE)
  - OpenAI Realtime (Requires API Key)
- **CSS**: `styles.css` (lines 1190-1224) - styled dropdown

### 5. Fixed Accessibility Issues
- **Issue**: Modal elements could receive focus when hidden
- **Solution**: Added `inert` attribute management
- **Files**: `sidepanel.html` line 59, `sidepanel.js` lines 237, 262

## ğŸš§ Remaining Implementation

### JavaScript Changes Needed in `sidepanel.js`

#### Step 1: Add Global Variables (after existing variables, ~line 100)
```javascript
// AI Provider Configuration
let aiProvider = 'groq'; // 'groq' or 'openai'
let browserRecognition = null; // Web Speech API recognition
let browserSynthesis = window.speechSynthesis;
let conversationHistory = []; // For Groq context
```

#### Step 2: Create Browser Speech Recognition System (~500 lines)

**Location**: After existing WebRTC functions, before settings functions

```javascript
// =============================================================================
// BROWSER SPEECH RECOGNITION (For Groq Mode)
// =============================================================================

function initBrowserSpeech() {
  if (!('webkitSpeechRecognition' in window)) {
    console.error('âŒ Browser Speech Recognition not supported');
    els.voiceStatus.textContent = 'Speech recognition not supported in this browser';
    return false;
  }

  const SpeechRecognition = window.webkitSpeechRecognition || window.SpeechRecognition;
  browserRecognition = new SpeechRecognition();

  browserRecognition.continuous = true;
  browserRecognition.interimResults = true;
  browserRecognition.lang = 'en-US';
  browserRecognition.maxAlternatives = 1;

  let finalTranscript = '';
  let interimTranscript = '';

  browserRecognition.onstart = () => {
    console.log('ğŸ¤ Browser speech recognition started');
    isListening = true;
    updateOrbState();
    els.voiceStatus.textContent = 'Listening...';
  };

  browserRecognition.onresult = (event) => {
    interimTranscript = '';

    for (let i = event.resultIndex; i < event.results.length; i++) {
      const transcript = event.results[i][0].transcript;

      if (event.results[i].isFinal) {
        finalTranscript += transcript + ' ';
        console.log('ğŸ¤ Final transcript:', transcript);
      } else {
        interimTranscript += transcript;
      }
    }

    // Show interim results in UI
    if (interimTranscript) {
      els.voiceStatus.textContent = `Hearing: "${interimTranscript}"`;
    }

    // Send to Groq when we have final transcript
    if (finalTranscript.trim()) {
      sendToGroq(finalTranscript.trim());
      finalTranscript = '';
    }
  };

  browserRecognition.onerror = (event) => {
    console.error('âŒ Speech recognition error:', event.error);

    if (event.error === 'no-speech') {
      els.voiceStatus.textContent = 'No speech detected';
    } else if (event.error === 'aborted') {
      els.voiceStatus.textContent = 'Recognition aborted';
    } else {
      els.voiceStatus.textContent = `Error: ${event.error}`;
    }

    isListening = false;
    updateOrbState();
  };

  browserRecognition.onend = () => {
    console.log('ğŸ¤ Browser speech recognition ended');
    isListening = false;
    updateOrbState();

    // Auto-restart in continuous mode
    if (continuousMode && connected) {
      setTimeout(() => {
        if (connected && !isMuted) {
          browserRecognition.start();
        }
      }, 100);
    }
  };

  return true;
}

async function sendToGroq(message) {
  try {
    console.log('ğŸ“¤ Sending to Groq:', message);
    els.voiceStatus.textContent = 'Thinking...';
    isSpeaking = true;
    updateOrbState();

    // Add user message to chat
    addMessageToChat('user', message);

    // Add to conversation history
    conversationHistory.push({
      role: 'user',
      content: message
    });

    // Keep only last 10 messages for context
    if (conversationHistory.length > 10) {
      conversationHistory = conversationHistory.slice(-10);
    }

    const response = await fetch(`${serverUrl}/api/groq`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        message: message,
        conversationHistory: conversationHistory.slice(0, -1) // Exclude current message
      })
    });

    if (!response.ok) {
      throw new Error(`Groq API failed: ${response.status}`);
    }

    const data = await response.json();
    const assistantMessage = data.message;

    console.log('âœ… Groq response:', assistantMessage);

    // Add assistant response to chat
    addMessageToChat('assistant', assistantMessage);

    // Add to conversation history
    conversationHistory.push({
      role: 'assistant',
      content: assistantMessage
    });

    // Speak the response using browser TTS
    speakText(assistantMessage);

  } catch (error) {
    console.error('âŒ Groq error:', error);
    els.voiceStatus.textContent = 'Error processing request';
    isSpeaking = false;
    updateOrbState();

    addMessageToChat('system', `Error: ${error.message}`);
  }
}

function speakText(text) {
  if (!browserSynthesis) {
    console.error('âŒ Speech synthesis not available');
    isSpeaking = false;
    updateOrbState();
    return;
  }

  // Cancel any ongoing speech
  browserSynthesis.cancel();

  const utterance = new SpeechSynthesisUtterance(text);
  utterance.lang = 'en-US';
  utterance.rate = 1.0;
  utterance.pitch = 1.0;
  utterance.volume = 1.0;

  utterance.onstart = () => {
    console.log('ğŸ”Š Speaking...');
    isSpeaking = true;
    updateOrbState();
    els.voiceStatus.textContent = 'Speaking...';
  };

  utterance.onend = () => {
    console.log('âœ… Speech complete');
    isSpeaking = false;
    updateOrbState();
    els.voiceStatus.textContent = connected ? 'Hold to talk' : 'Disconnected';
  };

  utterance.onerror = (event) => {
    console.error('âŒ Speech synthesis error:', event);
    isSpeaking = false;
    updateOrbState();
  };

  browserSynthesis.speak(utterance);
}

function stopBrowserSpeech() {
  if (browserRecognition) {
    browserRecognition.stop();
  }
  if (browserSynthesis) {
    browserSynthesis.cancel();
  }
  isListening = false;
  isSpeaking = false;
  updateOrbState();
}
```

#### Step 3: Modify Connect Function

**Location**: Find the `async function connect()` (around line 800)

**Replace the connection logic with**:
```javascript
async function connect() {
  if (connected) {
    console.log('Already connected');
    return;
  }

  try {
    connected = true;
    els.connectBtn.textContent = 'Disconnect';
    els.connectBtn.classList.add('connected');

    // Check which AI provider to use
    if (aiProvider === 'groq') {
      // Initialize browser speech recognition
      if (!initBrowserSpeech()) {
        throw new Error('Failed to initialize browser speech recognition');
      }

      els.voiceStatus.textContent = 'Connected - Hold to talk';
      console.log('âœ… Connected to Groq with browser speech');

      // Start recognition in continuous mode
      if (continuousMode) {
        browserRecognition.start();
      }
    } else {
      // Use existing OpenAI Realtime API connection
      await connectToOpenAI(); // Your existing function
    }

    updateOrbState();
  } catch (error) {
    console.error('Connection failed:', error);
    connected = false;
    els.connectBtn.textContent = 'Connect';
    els.connectBtn.classList.remove('connected');
    els.voiceStatus.textContent = `Error: ${error.message}`;
  }
}
```

#### Step 4: Modify Disconnect Function

**Location**: Find the `function disconnect()` (around line 900)

**Add at the beginning**:
```javascript
function disconnect() {
  if (!connected) return;

  // Stop browser speech if using Groq
  if (aiProvider === 'groq') {
    stopBrowserSpeech();
    conversationHistory = []; // Clear history
  } else {
    // Existing OpenAI cleanup
    if (pc) {
      pc.close();
      pc = null;
    }
    if (dc) {
      dc.close();
      dc = null;
    }
    if (micStream) {
      micStream.getTracks().forEach(track => track.stop());
      micStream = null;
    }
  }

  connected = false;
  // ... rest of existing disconnect code
}
```

#### Step 5: Modify Push-to-Talk Button Handlers

**Location**: Find the push-to-talk button event listeners (around line 4800)

**Wrap existing logic**:
```javascript
// Push to talk - Press
els.pushToTalkBtn.addEventListener('mousedown', () => {
  if (!connected) return;

  if (aiProvider === 'groq') {
    // Start browser recognition
    if (browserRecognition && !isListening) {
      browserRecognition.start();
    }
  } else {
    // Existing OpenAI logic
    sendClientEvent({
      type: 'input_audio_buffer.commit'
    });
  }
});

// Push to talk - Release
els.pushToTalkBtn.addEventListener('mouseup', () => {
  if (!connected) return;

  if (aiProvider === 'groq') {
    // Stop browser recognition
    if (browserRecognition && isListening) {
      browserRecognition.stop();
    }
  } else {
    // Existing OpenAI logic
    sendClientEvent({
      type: 'response.create'
    });
  }
});
```

#### Step 6: Add Settings Handler for AI Provider

**Location**: In the settings initialization section (around line 180)

```javascript
// AI Provider setting
els.aiProvider = document.getElementById('aiProvider');
if (els.aiProvider) {
  // Load saved preference
  chrome.storage.local.get(['aiProvider'], (result) => {
    aiProvider = result.aiProvider || 'groq';
    els.aiProvider.value = aiProvider;
    console.log('AI Provider loaded:', aiProvider);
  });

  // Save on change
  els.aiProvider.addEventListener('change', () => {
    aiProvider = els.aiProvider.value;
    chrome.storage.local.set({ aiProvider });
    console.log('AI Provider changed to:', aiProvider);

    // Disconnect and require reconnection with new provider
    if (connected) {
      disconnect();
      els.voiceStatus.textContent = 'Disconnected - Click Connect to use new provider';
    }
  });
}
```

## ğŸ“ Testing Checklist

After implementing the above:

1. âœ… Open extension settings
2. âœ… Select "Groq + Browser Speech (FREE)"
3. âœ… Click Connect button
4. âœ… Hold "Push to Talk" and speak
5. âœ… Verify transcription appears in chat
6. âœ… Verify Atlas responds via browser TTS
7. âœ… Test continuous mode
8. âœ… Test switching back to OpenAI mode

## ğŸ”§ Files Modified Summary

- âœ… `dev/server/server.js` - Added `/api/groq` endpoint
- âœ… `sidepanel.html` - Added AI provider dropdown
- âœ… `styles.css` - Added dropdown styling
- ğŸš§ `sidepanel.js` - Needs browser speech implementation (above)

## ğŸ¯ Expected Results

- **Cost**: $0 for normal usage (Groq free tier: 14,400 requests/day)
- **Speed**: ~0.3 seconds per response (10x faster than GPT-4)
- **Quality**: High (Llama 3.3 70B is comparable to GPT-3.5)
- **Offline**: Speech recognition and TTS work offline

## ğŸ“Š Current Status

- Backend: **100% Complete** âœ…
- UI: **100% Complete** âœ…
- JavaScript: **0% Complete** (needs implementation above)

## ğŸ”— Useful Links

- Groq Console: https://console.groq.com
- Groq Docs: https://console.groq.com/docs
- Web Speech API: https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API

## ğŸ’¡ Notes

- All API keys are already in Vercel
- Groq endpoint is tested and working
- Browser Speech API is built into Chrome (no installation needed)
- Conversation history limited to last 10 messages to keep context relevant
